\documentclass[11pt]{article}
\usepackage[top=1.0in,bottom=1.0in,right=1.0in,left=1.0in]{geometry}
\usepackage{amsmath}
\usepackage[english]{babel}
\usepackage{amsthm}
\newtheorem{defn}{Definition:}
\newtheorem{theo}{Theorem:}

\begin{document}
\title{Entropy and Mutual Information}
\author{M.Tech CS\\
 Information and Coding Theory}
\date {Before Mid Semester}
\maketitle
\section{Entropy}
\begin{defn}
Entropy is the minimum descriptive complexity of a random variable. In other word, it is measure of the uncertainty of a random variable.
\end{defn}
Formally,

If \textbf{ \textit{X}} be a discrete random variable over alphabet $\chi$ and probabilty mass function is
 
p(x)=Pr\{ {\textit{X} = \textit{x} \}= \textit{x}} \textit{x} $\in$ \textbf{\textit{$\chi$} }.

The Entropy \textit{H($\chi$)} of a discrete random variable \textbf{ \textit{X}} is defined by 

\begin{equation}
H(X) = - \sum \limits_{x \in \chi} p(x) log_{2} p(x)
\end{equation}


Entropy is expressed in bit. 
When entropy is measured in natural base ($\ln$) it is called \textit{nats}.\\
\textbf{Some useful properties}
\begin{enumerate}
	

\item $H(X) \geq 0$
\item $H_{b}(X)=(\log_{b} a H_{a} (X).$
\item $H(X)=-E\log p(x) = E \frac{1}{\log p(X)}$ where E($\bullet$) is expection of given function.

\end{enumerate}

\subsection{Joint Entropy and Conditional Entropy}

\begin{defn}
The \textbf{joint entropy } $H(X,Y)$ of a pair of discrete random variables $(X,Y)$ with joint distribution $p(x,y)$ is defined as 
\end{defn}


\begin{equation}
\begin{aligned}
H(X,Y) &= - \sum \limits_{x \in \chi} \sum \limits_{y \in \gamma} p(x,y)log_{2} p(x,y)
\end{aligned}
\end{equation}
\begin{equation}
\begin{aligned}
H(X,Y)&=-Elog_{2}p(X,Y)
\end{aligned}
\end{equation}

\begin{defn}
if (X,Y) $\sim$ p(x,y), then \textbf{conditional entropy} $H(Y|X)$ is defined as
\end{defn}

\begin{align*}
H(Y|X)&=\sum \limits_{x \in \chi} p(x) H(Y|X=x)\\
&=-\sum \limits_{x \in \chi} p(x) \sum \limits_{y \in \gamma} p(y|x)\log_{2} p(y|x)\\
&= - \sum \limits_{x \in \chi} \sum \limits{y \in \gamma} p(x,y) \log_{2} p(y|x)\\
&=-E \log_{2} p(Y|X)
\end{align*}

\begin{theo}
\textit{Chain Rule:}

\begin{align*}
H(X,Y)&= H(X) + H(Y|X)
\end{align*}

\begin{proof}
\begin{align*}
H(X,Y)&=-\sum \limits_{x \in \chi, y \in \gamma} p(x,y)\log_{2}p(x,y)\\
&=- \sum \limits_{x \in \chi} \sum \limits_{y \in \gamma} p(x,y) \log_{2} p(x)p(y|x)\\
&=-\sum \limits_{x \in \chi} \sum \limits_{y \in \gamma}p(x,y)\log_{2}p(x) -\sum \limits_{x \in \chi} \sum \limits_{y \in \gamma} p(x,y)\log_{2}p(y|x)\\
&=-\sum \limits_{x \in \chi} p(x)\log_{2}p(x) -\sum\limits_{x \in \chi} \sum \limits_{y \in \gamma} p(x,y)\log_{2}p(y|x)\\
&=H(X) + H(Y|X) 
\end{align*}
\end{proof}

\end{theo}

\subsection{Relative Entropy}
\begin{defn}
\textbf{Relative Entropy} or \textbf{Kullback-Liebler distance} between two probabilty mass functions $p(x)$ and $q(x)$ over alphabet $\chi$
\begin{equation}
D(p(x)||q(x))=\sum \limits_{x \in \chi} p(x) \log_{2}\frac{p(x)}{q(x)}
\end{equation}
\begin{equation}
D(p(x)||q(x))=E_{p(x)}\log_{2} \frac{p(X)}{q(X)}
\end{equation}

\end{defn}

\begin{verse}
When p(x) = q(x) for $ x \in \chi $\\
$D(p(x)||q(x))=0$
\end{verse}



\end{document}